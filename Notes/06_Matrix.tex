\include{includes}
%SetFonts


\title{Topic 6: Matrices}
\author{02-680: Essentials of Mathematics and Statistics}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

%%%%%
You can almost think of a \emph{matrix} as a 2-dimension vector. 
We say that an ``$n$-by-$m$'' matrix $M \in \reals^{n\times m}$ has $n$ rows and $m$ columns and we usually write it as:
\[
M = \left[\begin{matrix}
M_{1,1}& 	M_{1,2}& 	\dots& M_{1,m}\\
M_{2,1}& 	M_{2,2}& 	\dots& M_{2,m}\\ 
\vdots & \vdots & \ddots & \vdots \\ 
M_{n,1}& 	M_{n,2}& 	\dots& M_{n,m}
\end{matrix}\right]
\]



%%%%%%%%%
\section{Simple Matrix Operations}

\subsection{Addition and Scalar Multiplication.}
Like with vectors, addition of two matrices as well as scalar multiplication are element-wise operations, so for matrices $M,N \in \reals^{n\times m}$ and scalar $a\in\reals$:
\[O = M+N \rightarrow O_{i,j} = M_{i,j} + N_{i,j} \;\; \forall 1 \le i \le n, 1 \le j \le m\]
\[O = aM \rightarrow O_{i,j} = a M_{i,j} \;\; \forall 1 \le i \le n, 1 \le j \le m\]

\subsection{Transpose} 
For a given matrix $M \in \reals^{n\times m}$, the transpose $M^T \in \reals^{m \times n}$ is defined such that:
\[
\forall I\in[0,n-1], j\in[0,m-1] : M^T_{j,i} = M_{i,j} 
\]
This operation works for both matrixes and vectors (which are really $n\times1$ matrices).
Some examples: 
\begin{center}
$\left[\begin{matrix}
1 & 2 & 3\\
4 & 5 & 6
\end{matrix}\right]^T = 
\left[\begin{matrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{matrix}\right]$%
\hspace{5em}
$\left[\begin{matrix}
7 \\
8 \\
9 \\
10 \\
\end{matrix}\right]^T = \left[\begin{matrix}
7 & 8 & 9 & 10
\end{matrix}\right]
$
\end{center}
%%%%%%%%%
\section{Matrix Multiplication}
Just like with vectors, multiplying two matrices is more complicated than scalars. 
The first question is the size of the result, if we multiply $C \in \reals^{n\times p}$ with $D \in \reals^{p\times m}$ we get a matrix $E \in \reals^{n\times m}$;
notice that the \textit{inner} dimensions are the same.
And the values in $E$ are defined as follows:
\[
E_{i,j} = \sum_{k=1}^m C_{i,k}D{k,j}
\]

We can actually rewrite this using dot product, lets say that $C_{i,*}$ is the $i$-th column of $C$, and $D_{*,j}$ is the $j$-th column of $D$.
In that case \[E_{i,j} = C_{i,*}\cdot D_{*,j}^T.\]

What can we do with it? Lets define the following:
\begin{itemize}
\item $G$ is an $n$-by-$m$ matrix where $G_{i,j}=1$ if actor $i$ was in an episode of the show $j$ (and $0$ otherwise)
\item $H$ be an $m$-by-$p$ matrix where $H_{j,k}=1$ if the show $j$ is available to stream on service $k$ (and $0$ otherwise) 
\end{itemize}


%%%%%%%%%%
\section{Square Matrices}
Square matrices (that is, matrices where $m=n$) come up a lot, 
possibly because of this or vice versa there are several properties and operations that exist only on these. 

In a square matrix $N\in\reals^{n\times n}$, we define the \emph{main diagonal} as the entries where the horizontal and vertical component are equal; 
i.e. $\left\{N_{i,i} \mid 1 \le i \le n\right\}$. 

\paragraph{Symmetry.}
We say a square matrix is \emph{symmetric} if $A=A^T$
(and \textit{anti-symmetric} is $A = -A^T$). 
That is, $A$ is symmetric if it is mirrored across the main diagonal which often happens for things like distance matrices (though not always as we'll see). 
Similarly, it is anti-symmetric if it's mirrored across the \textit{anti-diagonal}.

\paragraph{Trace. }
The \emph{trace} of a matrix $tr(A)$ is the sum of the diagonal elements: \[tr(A) := \sum_{I=1}^n A_{i,i}.\] 
The trace does not change under transpose, and is distributive across sum and scalar product. 

\subsection{Identity Matrix}

The \emph{identity} matrix $I_n \in\reals^{n\times n}$ (sometimes simplified to just $I$ when the size is implied from context) 
is a special symmetric matrix where the main diagonal values are $1$ and all other values are $0$.
\[
\forall I,j \in [1,n]: I_{i,j} = \begin{cases} 1 & i=j\\ 0 & i\ne j\end{cases}
\]
Note, $I_n$ is symmetric and $tr(I_n)=n$.


\section*{Useful References}
Liben-Nowell, ``Connecting Discrete Mathematics and Computer Science, 2e''. \S 2.4\\
Wilder, ``10-606-f23:Lecture 3'' GitHub repository, \url{https://github.com/bwilder0/10606-f23/blob/main/files/notes_linalg.pdf}\\
Kolter, ``Linear Algebra Review and Reference'', \url{https://www.cs.cmu.edu/~zkolter/course/15-884/linalg-review.pdf} \S1.1,2.3,3.1-3.5

\end{document}